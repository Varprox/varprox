\section{Introduction}

% ---------------------------------------------------------------------------- %

\subsection{Contributions}

% ---------------------------------------------------------------------------- %

\subsection{Previous studies / State-of-the-art}
\label{ssec:prev_stud}


Liste de papiers intéressants:
\begin{itemize}
\item \cite{OLeary_D_2012_j-comput-optim-appl_variable_pnlsp}: variable projection
  with constraints on the variable but there is no regularization
\item \cite{Leeuwen_T_2021_j-siam-j-sci-comput_variable_pnp}: variable projection
  with regularization but without constraint.
  The formulation loses the structure of variable projection does not handle box
  constraint with regularization.
  ({\color{red} mais plus je lis moins, je comprends l'article et leur algorithme)}
  In contrast, we can handle both contraints and regularizations using
  primal-dual optimization.
\item J. Chen, M. Gan, G. -Y. Chen and C. L. P. Chen, ``Constrained Variable Projection Optimization for Stationary RBF-AR Models,'' in IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52, no. 3, pp. 1882-1890, March 2022, doi: 10.1109/TSMC.2020.3034644. \\
  Pas d'accès à l'I2M et pas de version preprint (article IEEE)
\item ``Constrained variable projection method for blind deconvolution''
  (doi: 10.1088/1742-6596/386/1/012005): not very interesting, variable
  projection with nonnegative constraints nad Tikhonov regularization
  (similar to what we do for topothesy $\tau$)
\item \TODO{à compléter}
\end{itemize}

% ---------------------------------------------------------------------------- %

\subsection{Organization of the paper}

% ---------------------------------------------------------------------------- %

\subsection{Notation}

% ============================================================================ %

\section{}

% ============================================================================ %

\section{}

\begin{equation}
  \label{eq:optim_pb}
  \begin{aligned}
    \min_{\bbeta,\btau} \quad &
    \frac{1}{2} \sum_{n=1}^{N} {\left(F_{n}(\bbeta)\btau - \tilde{w}_{n}\right)}^{2}
    +\frac{1}{2}\lambda_{\alpha}\norm{\btau}_{2}^{2} + \lambda_{\beta}\mR(\bbeta) \\
    \textrm{s.t.} \quad &
    \btau \in \RR^{M}_{+} \, , \, \bbeta \in ]0,1[^{M} \, .
  \end{aligned}
\end{equation}
where $\mR$ is a regularization that is proper, convex and lower-semicontinuous.
For instance, $\mR$ can be the total variation (TV) $\mR(\*x)=\norm{\*D\*x}_{1}$
with $\*D$ the discrete gradient matrix defined as the idendity matrix with
coefficients -1 on its subdiagonal.

Equation~\eqref{eq:optim_pb} has the typical form of a constrained variable
projection problem~\cite{Golub_G_2003_j-inv-prob_separable_nlsvpma}.
Indeed, it can be formulated as
\begin{equation}
  \begin{aligned}
    \min_{\bbeta,\btau} \quad &
    \frac{1}{2} \norm{\begin{pmatrix}\*F(\bbeta) \\ \sqrt{\lambda_{\alpha}}\Id\end{pmatrix}\btau
        - \begin{pmatrix}\bwtilde\\ \*0 \end{pmatrix}}_{2}^{2}
    + \lambda_{\beta}\mR(\bbeta) \\
    \textrm{s.t.} \quad &
    \btau \in \RR^{M}_{+} \, , \, \bbeta \in ]0,1[^{M} \, .
  \end{aligned}
\end{equation}

Solving Problem~\ref{eq:optim_pb} can be performed in a block-descent scheme.
Minimization with respect to $\btau$ consists in solving a non-negative
least-squares problem.
This problem has a long history and can be solved using one of the standard
methods.
For instance one can use the trust region reflective algorithm described
in~\cite{Coleman_T_1996_j-siam-j-optim_interior_tranmsb} and implemented in SciPy
library~\cite{Virtanen_P_2020_j-nat-meth_scipy_fascp}.
On the other hand, minimization with respect to $\bbeta$ can be performed with
primal-dual methods~\cite{Komodakis_N_2015_j-ieee-sig-proc-mag_playing_d}.
Indeed, setting $f$ as the indicator function of the set $[\epsilon, 1-\epsilon]$, $g$ as
the $\ell_{1}$-norm multiplied by $\lambda_{\beta}$, and $h$ as the least-squares term,
Problem~\ref{eq:optim_pb} can be written as an unconstrained problem of
the form
\begin{equation}
  \label{eq:uncons_beta_optim}
  \min_{\bbeta\in\mathbb{R}^{M}} f(\bbeta) + g(\*D\bbeta) + h(\bbeta) \, ,
\end{equation}
where $f$, $g$, and $h$ are proper, convex, and lower-semicontinuous functions,
$h$ is gradient $1$-Lipschitz, and $D$ is a linear operator from $\RR^{M}$ to
$\RR^{M}$.
The dual problem of~\eqref{eq:uncons_beta_optim} is hence given by
\begin{equation}
  \label{eq:dual_beta_optim}
  \min_{\*v\in\mathbb{R}^{M}} (f^{*} \iconv h^{*})(\*L^{\top}\*v) + g^{*}(\*v) \, ,
\end{equation}
where $\*v$ is the dual variable, ${}^{*}$ denotes the Fenchel transform such
that $f^{*}(\*v)=\sup_{\*x\in\RR^{M}}(\scalar{\*x}{\*v}-f(\*x))$, and
$\iconv$ is the infinimal convolution defined by
$(f \iconv g)(\*x) = \inf_{\*y\in\RR^{M}} f(\*y) + g(\*x-\*y)$.
Note that the infinimal convolution acts to the Fenchel transform as the classic
convolution to the Fourier transform~\cite{Komodakis_N_2015_j-ieee-sig-proc-mag_playing_d}.
For instance, the two following equalities hold:
${(f \iconv g)}^{*} = f^{*} + g^{*}$ and ${(f+g)}^{*} = f^{*} \iconv g^{*}$.
While in Fourier analysis we are interested in transform of products and
convolutions, in convex analysis, we are interested in the transform of sum
since our objective function is often composed by a sum of several terms.
The main idea of primal-dual algorithms consists then to solve both the
primal~\eqref{eq:uncons_beta_optim} and the dual~\eqref{eq:dual_beta_optim}
problems at the same time.
A parallel to solving signal processing problems using time-frequency approach
can be made.
Exploiting information of both problems allows primal-dual techniques to yield
computational advantages as well as to tackle more general problems.

In order to solve Problem~\eqref{eq:uncons_beta_optim} (and
consequently~\eqref{eq:dual_beta_optim}), we propose to use the Rescaled Primal
Dual Forward-Backward (RFBPD)~\cite{Komodakis_N_2015_j-ieee-sig-proc-mag_playing_d}
whose iteration scheme is displayed in Equation~\eqref{eq:rfbpd}
\begin{equation}
  \label{eq:rfbpd}
  \begin{aligned}
    \*p_{n} &= \textrm{prox}_{\rho f} (\bbeta_{n}-\rho(\nabla h(\bbeta_{n})+\sigma \*D^{\top}\*v_{n})) \\
    \*q_{n} &= (\mathrm{Id}-\textrm{prox}_{\lambda g/\sigma}) (\*v_{n}+\*D(2\*p_{n}-\bbeta_{n})) \\
    (\*\bbeta_{n+1},\*v_{n+1}) &= (\bbeta_{n},\*v_{n}) + \omega_{n}((\*p_{n},\*q_{n})-(\bbeta_{n},\*v_{n}))
    \, ,
  \end{aligned}
\end{equation}
where $\omega$ is a positive relaxation or inertial factor, and $\tau$ and $\sigma$ are the
two step-sizes.
Noting that $\mathrm{Id}-\textrm{prox}_{\lambda g/\sigma}$ is, up to a rescaling, equal to
$\prox_{\sigma g^{*}}$, the iteration~\eqref{eq:rfbpd} closely looks like a
Forward-Backward (FB) step performs on the primal, followed by a FB step
performs on the dual.

The implementation of the RFBPD algorithm to solve~\eqref{eq:optim_pb} is given
in Algorithm~\ref{algo:rfbpd} where $\Soft_{\gamma}$ is the soft thresholding operator
with positive parameter $\gamma$ applied element-wise
$\Soft_{\gamma}(x) = \sgn(x)\max(0,\abs{x}-\gamma)$.
%% Thanks to the primal dual approach, we can separate the operator $\*D$ and the
%% $\ell_{1}$ in the computation of the proximal operator of the TV regularization and we
%% obtain a closed form expression.

\begin{algorithm}[t]
  \small
  \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE{Initial value of $\bbeta_{0}$}
    \REQUIRE{$(\tau,\sigma)\in]0,+\infty[^{2}$, and $\omega\in\RR_{+}$.}
    \ENSURE{Estimate of $\bbeta$.}
    \STATE{Initialize $i$ to $0$.}
    \STATE{Initialize $\*v$ to $\*0$.}
    \REPEAT{}
    \STATE{Primal update:
      \[
      \begin{aligned}
        \*p &= \bbeta - \tau\nabla g(\bbeta) - \sigma \*D^{\top}\*v \\
        \*p &= \Pi_{[\epsilon,1-\epsilon]^{M}}(\*p)
      \end{aligned}
      \]}
    \STATE{Dual update:
      \[
      \*q = (\Id - \Soft_{\lambda_{\beta}/\sigma})(\*v+\*S(2\*p-\bbeta))
      \]}
    \STATE{Inertial update:
      \[
      \begin{aligned}
        \bbeta &= \bbeta + \omega(\*p-\bbeta) \\
        \*v &= \*v + \omega(\*q - \*v)
      \end{aligned}
      \]}
    \STATE{Increment $i$.}
    \UNTIL{stopping criterion is met}
    \RETURN{$\bbeta$}
  \end{algorithmic}
  \caption{Implementation of RFBPD to solve~\eqref{eq:optim_pb} in
    $\bbeta$~\label{algo:rfbpd}}
\end{algorithm}

% ============================================================================ %

\section{}

% ============================================================================ %

\section{Conclusion}
\label{sec:concl}

Conclusion
